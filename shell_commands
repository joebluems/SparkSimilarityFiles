import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}
import spark.implicits._
import org.apache.spark.sql.functions.regexp_replace
import org.apache.spark.ml.linalg.SparseVector
import org.apache.spark.sql.functions._
import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover}
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.types._
val hashSize = 1000 //size of vocabulary hash

/// UDF for Pre-calculating sparse vector norm ///
def calcNorm(vectorA: SparseVector): Double = {
  var norm = 0.0
  for (i <-  vectorA.indices){ norm += vectorA(i)*vectorA(i) }
  (math.sqrt(norm))
}
val calcNormDF = udf[Double,SparseVector](calcNorm)

/// UDF for COSINE SIMILARIITY ////////////
def cosineSimilarity(vectorA: SparseVector, vectorB:SparseVector,normASqrt:Double,normBSqrt:Double) :(Double) = {
  var dotProduct = 0.0
  for (i <-  vectorA.indices){ dotProduct += vectorA(i) * vectorB(i) }
  val div = (normASqrt * normBSqrt)
  if( div == 0 ) (0)
  else (dotProduct / div)
}
val calcCosine = udf[Double,SparseVector,SparseVector,Double,Double](cosineSimilarity)

def getStopWords: Array[String] = {
    StopWordsRemover.loadDefaultStopWords("english") ++
      Seq("till", "since")
}

def getAbbreviations: Seq[String] =
  //TODO Add more if needed
  Seq(
      """(?i)\w+'ll""",
      """(?i)\w+'re""",
      """(?i)\w+'ve""",
      """(?i)\w+'s""",
      """(?i)i'm""",
      """(?i)\w+'t"""
  )

def removeAbbreviations(rawText: DataFrame, noAbbrColumn: String): DataFrame = {
    val zeroDf = rawText.withColumn(noAbbrColumn, col("contents"))
    getAbbreviations
      .foldLeft(zeroDf) { (df, abbr) =>
        df.withColumn(noAbbrColumn, regexp_replace(col(noAbbrColumn), abbr, ""))
      }
}

def prepareWords(rawText: DataFrame, wordsColumn: String): DataFrame = {
    val noAbbrColumn = "no_abbr"
    val noAbbrText = removeAbbreviations(rawText, noAbbrColumn)

    val noPunctColumn = "no_punct"
    val noPunctText = noAbbrText
      .withColumn(noPunctColumn,
        regexp_replace(col(noAbbrColumn), """[\p{Punct}]""", ""))

    val rawWordsColumn = "raw_words"
    val tokenizer: RegexTokenizer = new RegexTokenizer()
      .setInputCol(noPunctColumn)
      .setOutputCol(rawWordsColumn)
      .setToLowercase(true)
    val rawWords = tokenizer.transform(noPunctText)
      .where(size(col(rawWordsColumn)) > 0)

    val stopWordsRemover = new StopWordsRemover()
      .setInputCol(rawWordsColumn)
      .setOutputCol(wordsColumn)
      .setStopWords(getStopWords)
    stopWordsRemover.transform(rawWords)
}




///////////// TRANSFORM CORPUS & TRAIN TF-IDF //////////////
// load data into DF & tokenize 
val rawdata = spark.sparkContext.wholeTextFiles("./documents").toDF("filename","contents")
val wordsData = prepareWords(rawdata, "words")

// create hash and train the IDF model  - the IDF model should be written to file
val hashingTF = new HashingTF().setInputCol("words").setOutputCol("rawFeatures").setNumFeatures(hashSize)
val featurizedData = hashingTF.transform(wordsData)
val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features")
val idfModel = idf.fit(featurizedData)

// apply TF-IDF to corpus, add norm - the corpus is now ready for comparison
val rescaledData = idfModel.transform(featurizedData)
val normalized = rescaledData.withColumn("norm",calcNormDF(col("features")))
normalized.select("words","features","norm").show()


///////////// FINDING SIMILAR DOCUMENTS //////////////
// read in a new document & format ....
val newrawdata = spark.sparkContext.wholeTextFiles("./test").toDF("filename2","contents")
newrawdata.collect.foreach(println)
val newWords = prepareWords(newrawdata, "words")
val newFeature = hashingTF.transform(newWords)
val newRescale = idfModel.transform(newFeature).withColumnRenamed("features", "features2").withColumnRenamed("contents", "contents2")
val newNormal = newRescale.withColumn("norm2",calcNormDF(col("features2")))
newNormal.select("words","features2","norm2").show()

//// cross join corpus with new text & show top 5 similar ////
val cross = newNormal.crossJoin(normalized)
val cosine = cross.withColumn("similarity",calcCosine(col("features"),col("features2"),col("norm"),col("norm2")))
cosine.sort(desc("similarity")).select("similarity","filename").withColumn("similarity",format_number($"similarity",4)).show(5,false)

